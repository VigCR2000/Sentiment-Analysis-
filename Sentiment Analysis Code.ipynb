{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YB4ItAc3mWR4"
      },
      "outputs": [],
      "source": [
        "#importing all the libraries\n",
        "from newsapi import NewsApiClient\n",
        "from bs4 import BeautifulSoup\n",
        "import json, xml\n",
        "import requests\n",
        "import html\n",
        "import re\n",
        "import xml.etree.ElementTree\n",
        "import itertools\n",
        "import os, tempfile, gcsfs\n",
        "import newspaper\n",
        "import pathlib\n",
        "from transformers import BertTokenizer, BertModel, pipeline\n",
        "import torch\n",
        "from summarizer import Summarizer\n",
        "from collections import Counter\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0G-ZsAvDDxF"
      },
      "outputs": [],
      "source": [
        "# De-contracting english phrases\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return str(phrase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqg_etyVDEmn"
      },
      "outputs": [],
      "source": [
        "#text cleaning\n",
        "def clean_text(text):\n",
        "    #Clean the html tags\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/.\\S+', \"\", text)\n",
        "\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = re.sub(r'\\\"', '', text)\n",
        "    text = re.sub(r'’','\\'',text)\n",
        "    text = re.sub(r'”','',text)\n",
        "    text = re.sub(r'“','',text)\n",
        "\n",
        "    # remove old style retweet text \"RT\"\n",
        "    text = re.sub(r'^RT[\\s]+', '', text)\n",
        "\n",
        "    #One letter in a word should not be present more than twice in continuation\n",
        "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
        "\n",
        "    return decontracted(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQxzT1kDDdxk"
      },
      "outputs": [],
      "source": [
        "#removing tags\n",
        "\n",
        "def remove_tags(text):\n",
        "    try:\n",
        "        output = ''.join(xml.etree.ElementTree.fromstring(text).itertext())\n",
        "    except Exception:\n",
        "        output = text\n",
        "    return output\n",
        "\n",
        "def basicScrapper(url):\n",
        "    try:\n",
        "        article = newspaper.Article(url=url, language='en')\n",
        "        article.download()\n",
        "        article.parse()\n",
        "    except Exception as e:\n",
        "        print(f'Error in Basic Scrapper using NewsPaper3K: {e}')\n",
        "        return ''\n",
        "    return str(article.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmRQYB53M3gb"
      },
      "outputs": [],
      "source": [
        "#text chunking\n",
        "def chunk_text(text, max_length=512):\n",
        "    chunks = []\n",
        "    words = text.split()\n",
        "    current_chunk = ''\n",
        "    for word in words:\n",
        "        if len(current_chunk) + len(word) < max_length:\n",
        "            current_chunk += ' ' + word\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = word\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks[:512]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UuZjgFYDgSz"
      },
      "outputs": [],
      "source": [
        "#API call for articles\n",
        "num_art = 30\n",
        "def getNews(company, writeCloud=False, proxies=''):\n",
        "    apikey = '126f8f56e1634dbc862a85d391a21fe4'#'5df82aee71714503b341fb5046e5bf55'#'cd51b89890444ff4b367a6e14cbac7a8'#'769053c5c1c4424aa08588285b547c22'\n",
        "    keys = [apikey]\n",
        "    for i in range(len(keys)):\n",
        "        try:\n",
        "            key = keys[i]\n",
        "            print(f'Trying API Key {i+1} for NewsAPI request: {key}')\n",
        "            newsapi = NewsApiClient(api_key = key)\n",
        "            #news = newsapi.get_top_headlines(category='business', language='en', country = 'us')\n",
        "            break\n",
        "        except Exception as e:\n",
        "            if not e.args[0]['code'] == 'rateLimited':\n",
        "                raise Exception(f'NewsAPI Error!! {e}')\n",
        "\n",
        "    company = company.strip()\n",
        "    queries = ['Stock Market', 'Stocks', 'Financial Markets', 'Equity Markets', 'Trading',\n",
        "    'Stock Exchange', 'Share Prices', 'Market Volatility',\n",
        "    'Market Indices (e.g., S&P 500, Dow Jones Industrial Average, NASDAQ)',\n",
        "    'Stock Performance', 'Market Trends', 'Economic Indicators', 'Corporate Earnings',\n",
        "    'Investment Strategies', 'Market Analysis', 'Market Outlook',\n",
        "    'Initial Public Offering (IPO)', 'Dividends', 'Market Sentiment', 'Market News',\n",
        "    'MetaTrader', 'NVIDIA', 'Microsoft', 'Amazon', 'Google',\n",
        "    'Wall Street', 'Bull Market', 'Bear Market', 'Blue Chip Stocks',\n",
        "    'Penny Stocks', 'Futures Market', 'Options Market', 'ETFs', 'Mutual Funds',\n",
        "    'Hedge Funds', 'Technical Analysis', 'Fundamental Analysis', 'Day Trading',\n",
        "    'Swing Trading', 'Long-term Investing', 'Short Selling', 'Market Capitalization',\n",
        "    'Market Liquidity', 'Market Order', 'Limit Order', 'Stop Order', 'Margin Trading',\n",
        "    'Risk Management''META',\n",
        "    'NVDA',\n",
        "    'MSFT',\n",
        "    'AMZN',\n",
        "    'GOOG'\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "    output_obj = []\n",
        "    titles = []\n",
        "\n",
        "    for query in queries:\n",
        "        news = newsapi.get_top_headlines(q=(company+' '+query).strip(), category='business', language='en', country='us')\n",
        "        for article in news['articles']:\n",
        "            if article['title'] not in titles and company.lower() in article['title'].lower():\n",
        "                del article['source'], article['author'], article['urlToImage']\n",
        "                if len(titles) == num_art:\n",
        "                    break\n",
        "                article['content'] = getFullArticleContent(company=company, url = article['url'], pre_content = article['content'], proxies=proxies)\n",
        "                if len(article['content'])!= 0 and len(article['description'])!= 0:\n",
        "                    output_obj.append(article)\n",
        "                    titles.append(article['title'])\n",
        "\n",
        "    if not len(titles) == num_art:\n",
        "        news = newsapi.get_everything(q=(company).strip(), language='en', from_param='2024-03-24', to='2024-04-03')\n",
        "        for article in news['articles']:\n",
        "            if article['title'] not in titles and company.lower() in article['title'].lower():\n",
        "                del article['source'], article['author'], article['urlToImage']\n",
        "                if len(titles) == num_art:\n",
        "                    break\n",
        "                article['content'] = getFullArticleContent(company=company, url = article['url'], pre_content = article['content'], proxies=proxies)\n",
        "                if len(article['content'])!= 0 and len(article['description'])!= 0:\n",
        "                    output_obj.append(article)\n",
        "                    titles.append(article['title'])\n",
        "\n",
        "    if not len(titles) == num_art:\n",
        "        for article in news['articles']:\n",
        "            if len(titles) == num_art:\n",
        "                break\n",
        "            if article['title'] not in titles and company.lower() in article['title'].lower():\n",
        "                content = basicScrapper(article['url'])\n",
        "                if len(content) == 0:\n",
        "                    article['content'] = clean_text(remove_tags(article['content'].split('… [')[0]))\n",
        "                else:\n",
        "                    article['content'] = content\n",
        "                if len(article['content'])!= 0 and len(article['description'])!= 0:\n",
        "                    output_obj.append(article)\n",
        "                    titles.append(article['title'])\n",
        "    # Initialize BERT Tokenizer and Model\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    bert_model = Summarizer()\n",
        "\n",
        "    for article in output_obj:\n",
        "      article_text = article['title'] + ' ' + article['description'] + ' ' + article['content']\n",
        "      # Split text into chunks\n",
        "      chunks = chunk_text(article_text)\n",
        "      # Initialize list to store summarized chunks\n",
        "      summarized_chunks = []\n",
        "      # Use BERT Summarizer on each chunk\n",
        "      for chunk in chunks:\n",
        "        summarized_chunk = bert_model(chunk, min_length=60, max_length=200)\n",
        "        summarized_chunks.append(summarized_chunk)\n",
        "    # Concatenate summarized chunks to get final summary\n",
        "    summary = ' '.join(summarized_chunks)\n",
        "    article['summary'] = summary\n",
        "\n",
        "    # Initialize the sentiment analysis pipeline outside the loop\n",
        "    sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "    for article in output_obj:\n",
        "      # Combine title, description, and content into a single text\n",
        "      text = article['title'] + ' ' + article['description'] + ' ' + article['content']\n",
        "      # Split the text into chunks\n",
        "      chunks = chunk_text(text)\n",
        "      sentiment_labels = []\n",
        "      # Perform sentiment analysis on each chunk\n",
        "      for chunk in chunks:\n",
        "        result = sentiment_pipeline(chunk)\n",
        "        sentiment_labels.extend([res['label'] for res in result])\n",
        "      # Store the sentiment labels for each chunk\n",
        "      sentiment_counter = Counter(sentiment_labels)\n",
        "      majority_sentiment_label = sentiment_counter.most_common(1)[0][0]\n",
        "      article['sentiment'] = majority_sentiment_label\n",
        "    return titles, output_obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woiiuuh8DsTw"
      },
      "outputs": [],
      "source": [
        "#content to retrieve from API\n",
        "def getFullArticleContent(company, url, pre_content='', proxies=''):\n",
        "    if not pre_content:\n",
        "        pre_content=''\n",
        "    content = ''\n",
        "    try:\n",
        "        response = requests.get(url, proxies=proxies)\n",
        "    except Exception:\n",
        "        print(f'URL not reachable: {url}')\n",
        "        return ''\n",
        "    if response.status_code == 200:\n",
        "        body = response.content\n",
        "        soup1 = BeautifulSoup(body, 'html.parser')\n",
        "        news = soup1.find_all('script')\n",
        "        for article in news:\n",
        "            try:\n",
        "                if article.has_attr('type'):\n",
        "                    if 'json' in article['type']:\n",
        "                        obj = json.loads(article.contents[0])\n",
        "                        if '@type' in obj.keys():\n",
        "                            if obj['@type'] ==  'NewsArticle':\n",
        "                                content+=' '+ str(obj[\"articleBody\"])\n",
        "            except Exception:\n",
        "                pass\n",
        "        news_div = soup1.find_all('div')\n",
        "        for div in news_div:\n",
        "            try:\n",
        "                paras = div.find_all('p')\n",
        "                for para in paras:\n",
        "                    data = str(remove_tags(str(para)))\n",
        "                    if company.lower() in data.lower() and data.lower() not in content.lower():\n",
        "                        content+=' '+data\n",
        "            except Exception:\n",
        "                pass\n",
        "        if len(content) == 0:\n",
        "            news_div = soup1.find_all('div', class_='article-text')\n",
        "            for article in news_div:\n",
        "                try:\n",
        "                    content+=' '+ str(remove_tags(str(article)))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            news_div = soup1.find_all('div', class_='article-content')\n",
        "            for article in news_div:\n",
        "                try:\n",
        "                    paragraphs = article.find_all('p')\n",
        "                    for para in paragraphs:\n",
        "                        content+=' '+ str(remove_tags(str(para)))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            news_div = soup1.find_all('div', class_='entry-content clearfix')\n",
        "            for article in news_div:\n",
        "                try:\n",
        "                    paragraphs = article.find_all('p')\n",
        "                    for para in paragraphs:\n",
        "                        content+=' '+ str(remove_tags(str(para)))\n",
        "                except Exception:\n",
        "                    pass\n",
        "    else:\n",
        "        content= ''\n",
        "    if len(content) == 0:\n",
        "        content = ''\n",
        "    return clean_text(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CK4Vi3pHDyLF"
      },
      "outputs": [],
      "source": [
        "#META data retreival\n",
        "if __name__=='__main__':\n",
        "    # Generate an API key from https://newsapi.org/register\n",
        "    #companies = ['Apple','Microsoft','Amazon','Walmart','Alphabet','Meta','Tesla','NVIDIA','Pfizer','Netflix']\n",
        "    #companies = ['Meta','Nvidia','Microsoft','Amazon','Google']\n",
        "    companies = ['Meta']\n",
        "    data = []\n",
        "    k = 1\n",
        "    for company in companies:\n",
        "        urls, output_objs = getNews(company=company,  writeCloud=False)\n",
        "        for url, article in zip(urls, output_objs):\n",
        "            data.append({\n",
        "                'Serial Number': k,\n",
        "                'Company': company,\n",
        "                'URL': url,\n",
        "                'Title': article['title'],\n",
        "                'Description': article['description'],\n",
        "                'Content': article['content'],\n",
        "                'published_at' : article['publishedAt'],\n",
        "                'Sentiment': article['sentiment']\n",
        "            })\n",
        "            k = k + 1\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('/Users/Desktop/ie 517 final project/untitled folder/finalMETA.csv', mode='a', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHDOHMRtgs7e"
      },
      "outputs": [],
      "source": [
        "#Nvidia data retreival\n",
        "if __name__=='__main__':\n",
        "    # Generate an API key from https://newsapi.org/register\n",
        "    #companies = ['Apple','Microsoft','Amazon','Walmart','Alphabet','Meta','Tesla','NVIDIA','Pfizer','Netflix']\n",
        "    #companies = ['Meta','Nvidia','Microsoft','Amazon','Google']\n",
        "    companies = ['Nvidia']\n",
        "    data = []\n",
        "    k = 1\n",
        "    for company in companies:\n",
        "        urls, output_objs = getNews(company=company,  writeCloud=False)\n",
        "        for url, article in zip(urls, output_objs):\n",
        "            data.append({\n",
        "                'Serial Number': k,\n",
        "                'Company': company,\n",
        "                'URL': url,\n",
        "                'Title': article['title'],\n",
        "                'Description': article['description'],\n",
        "                'Content': article['content'],\n",
        "                'published_at' : article['publishedAt'],\n",
        "                'Sentiment': article['sentiment']\n",
        "            })\n",
        "            k = k + 1\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('/Users/Desktop/ie 517 final project/untitled folder/finalNVIDIA.csv', mode='a', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlxeBHmtgs7e"
      },
      "outputs": [],
      "source": [
        "#Microsoft data retreival\n",
        "if __name__=='__main__':\n",
        "    # Generate an API key from https://newsapi.org/register\n",
        "    #companies = ['Apple','Microsoft','Amazon','Walmart','Alphabet','Meta','Tesla','NVIDIA','Pfizer','Netflix']\n",
        "    #companies = ['Meta','Nvidia','Microsoft','Amazon','Google']\n",
        "    companies = ['Microsoft']\n",
        "    data = []\n",
        "    k = 1\n",
        "    for company in companies:\n",
        "        urls, output_objs = getNews(company=company,  writeCloud=False)\n",
        "        for url, article in zip(urls, output_objs):\n",
        "            data.append({\n",
        "                'Serial Number': k,\n",
        "                'Company': company,\n",
        "                'URL': url,\n",
        "                'Title': article['title'],\n",
        "                'Description': article['description'],\n",
        "                'Content': article['content'],\n",
        "                'published_at' : article['publishedAt'],\n",
        "                'Sentiment': article['sentiment']\n",
        "            })\n",
        "            k = k + 1\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('/Users/Desktop/ie 517 final project/untitled folder/finalMSFT.csv', mode='a', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH6o25c8gs7e"
      },
      "outputs": [],
      "source": [
        "#Amazon data retreival\n",
        "if __name__=='__main__':\n",
        "    # Generate an API key from https://newsapi.org/register\n",
        "    #companies = ['Apple','Microsoft','Amazon','Walmart','Alphabet','Meta','Tesla','NVIDIA','Pfizer','Netflix']\n",
        "    #companies = ['Meta','Nvidia','Microsoft','Amazon','Google']\n",
        "    companies = ['Amazon']\n",
        "    data = []\n",
        "    k = 1\n",
        "    for company in companies:\n",
        "        urls, output_objs = getNews(company=company,  writeCloud=False)\n",
        "        for url, article in zip(urls, output_objs):\n",
        "            data.append({\n",
        "                'Serial Number': k,\n",
        "                'Company': company,\n",
        "                'URL': url,\n",
        "                'Title': article['title'],\n",
        "                'Description': article['description'],\n",
        "                'Content': article['content'],\n",
        "                'published_at' : article['publishedAt'],\n",
        "                'Sentiment': article['sentiment']\n",
        "            })\n",
        "            k = k + 1\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('/Users/Desktop/ie 517 final project/untitled folder/finalAMZN.csv', mode='a', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrNUDWQCgs7f"
      },
      "outputs": [],
      "source": [
        "#Google data retreival\n",
        "if __name__=='__main__':\n",
        "    # Generate an API key from https://newsapi.org/register\n",
        "    #companies = ['Apple','Microsoft','Amazon','Walmart','Alphabet','Meta','Tesla','NVIDIA','Pfizer','Netflix']\n",
        "    #companies = ['Meta','Nvidia','Microsoft','Amazon','Google']\n",
        "    companies = ['Google']\n",
        "    data = []\n",
        "    k = 1\n",
        "    for company in companies:\n",
        "        urls, output_objs = getNews(company=company,  writeCloud=False)\n",
        "        for url, article in zip(urls, output_objs):\n",
        "            data.append({\n",
        "                'Serial Number': k,\n",
        "                'Company': company,\n",
        "                'URL': url,\n",
        "                'Title': article['title'],\n",
        "                'Description': article['description'],\n",
        "                'Content': article['content'],\n",
        "                'published_at' : article['publishedAt'],\n",
        "                'Sentiment': article['sentiment']\n",
        "            })\n",
        "            k = k + 1\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('/Users/Desktop/ie 517 final project/untitled folder/finalGOOGLE.csv', mode='a', index=False, header=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
